import streamlit as st
from openai import OpenAI

# Configura√ß√£o do cliente LM Studio
client = OpenAI(base_url="http://localhost:8000/v1", api_key="lm-studio")

# Inicializa o hist√≥rico de mensagens
if "history" not in st.session_state:
    st.session_state["history"] = [
        {"role": "system", "content": """Voc√™ √© um assistente especializado em toadas de boi-bumb√° e cultura amaz√¥nica brasileira.
         Voce conhece profundamente os bois garantido, e o boi Caprichoso, e o festival de Parintins
         Voc√™ tamb√©m sabe criar musicas referentes √†s toadas. SEMPRE que o usu√°rio pedir uma musica, fa√ßa com pelo menos 4 versos referentes ao boi requerido."""},
        {"role": "assistant", "content": "Ol√°! Sou um assistente que conhece profundamente as toadas de boi-bumb√° e a cultura amaz√¥nica. Como posso ajudar hoje?"}
    ]

# Configura√ß√£o da p√°gina
st.title("Chatbot de Toadas Amaz√¥nicas üéµ")
st.subheader("Converse sobre a cultura amaz√¥nica e toadas de boi-bumb√°!")

# Exibe o hist√≥rico de conversas
def render_chat():
    for message in st.session_state["history"]:
        if message["role"] == "user":
            st.markdown(f"**Voc√™:** {message['content']}")
        elif message["role"] == "assistant":
            st.markdown(f"**Assistente:** {message['content']}")

# Caixa de entrada de mensagens
def send_message():
    user_input = st.session_state.input_text.strip()
    if user_input:
        # Armazena a mensagem do usu√°rio
        st.session_state["history"].append({"role": "user", "content": user_input})


        # Solicita a resposta do assistente
        with st.spinner("Gerando resposta..."):
            completion = client.chat.completions.create(
                model="bois/unsloth",
                messages=st.session_state["history"],
                temperature=0.7,
                stream=True,
            )

            new_message = {"role": "assistant", "content": ""}

            # Exibe tokens em tempo real
            placeholder = st.empty()
            for chunk in completion:
                if chunk.choices[0].delta.content:
                    new_message["content"] += chunk.choices[0].delta.content
                    placeholder.markdown(f"**Assistente:** {new_message['content']}")

            st.session_state["history"].append(new_message)

        # Limpa a entrada de texto de forma segura
        st.session_state["input_text"] = ""  # Evita modificar o widget diretamente
        placeholder.empty()  # Limpa o placeholder ao final da resposta

# Renderiza o hist√≥rico inicial
render_chat()

# Entrada de texto do usu√°rio
st.text_input(
    "Digite sua mensagem:", 
    key="input_text", 
    placeholder="Fale sobre uma toada cl√°ssica ou pergunte algo!",
    on_change=send_message
)
